{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM仿写莎士比亚十四行诗\n",
    "\n",
    "**任务要求：利用LSTM网络实现莎士比亚风格的十四行诗文本生成。请勿使用Copilot等深度学习代码提示工具完成此次作业。**  \n",
    "**数据: shakespeare_sonnets.txt**  \n",
    "**你将学会：**\n",
    "1. 对文本进行单词级别的编码和解码\n",
    "2. 以LSTM网络为核心部件的神经网络的定义和使用\n",
    "3. 文本生成任务的基本训练流程（基于交叉熵损失函数）\n",
    "4. 文本生成网络的基本采样流程\n",
    "\n",
    "导入必要的库，若缺少库请自行安装。其中`simple_tokenizer`是文件夹中定义好的模块文件，无需安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\34323\\vs_python\\pytest\\rnn练习\\.ipynb_checkpoints\\sonnets_lstm-checkpoint.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/.ipynb_checkpoints/sonnets_lstm-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/.ipynb_checkpoints/sonnets_lstm-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/.ipynb_checkpoints/sonnets_lstm-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msimple_tokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m tokenize, detokenize\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/.ipynb_checkpoints/sonnets_lstm-checkpoint.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'simple_tokenizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_tokenizer import tokenize, detokenize\n",
    "from einops import rearrange,repeat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 设置运输设备\n",
    "use_gpu = True if torch.cuda.is_available() else False # 如果GPU显存不够，可以手动设置为False\n",
    "device_id = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print('Device:', device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据导入和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sonnet:\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but, as the riper should by time decease,\n",
      "his tender heir might bear his memory.\n",
      "but thou, contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thyself thy foe, to thy sweet self too cruel.\n",
      "thou that art now the world's fresh ornament\n",
      "and only herald to the gaudy spring\n",
      "within thine own bud buriest thy content\n",
      "and, tender churl, mak'st waste in niggarding.\n",
      "pity the world, or else this glutton be--\n",
      "to eat the world's due, by the grave and thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sonnet_list = [] # 列表中的每一个元素是一首诗的字符串\n",
    "with open('shakespeares_sonnets.txt', 'r') as f:\n",
    "    while True:\n",
    "        text = ''\n",
    "        line1 = f.readline() # ignore the first line\n",
    "        if not line1:\n",
    "            break\n",
    "        assert eval(line1) == len(sonnet_list) + 1\n",
    "        line2 = f.readline() # ignore the second line\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if line == '\\n' or not line:\n",
    "                break\n",
    "            text += line.lower().strip() + '\\n'\n",
    "        sonnet_list.append(text)\n",
    "print('first sonnet:\\n%s' % sonnet_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用在`simple_tokenizer.py`中定义好的`tokenizer`函数把文本分割成单词列表，其中每个单词称为一个token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in first sonnet:  145\n",
      "['Afrom', 'Afairest', 'Acreatures', 'Awe', 'Adesire', 'Aincrease', 'S,', 'S\\n', 'Athat', 'Athereby']\n",
      "Reconstructed text:\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but, as the riper should by time decease,\n",
      "his tender heir might bear his memory.\n",
      "but thou, contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thyself thy foe, to thy sweet self too cruel.\n",
      "thou that art now the world's fresh ornament\n",
      "and only herald to the gaudy spring\n",
      "within thine own bud buriest thy content\n",
      "and, tender churl, mak'st waste in niggarding.\n",
      "pity the world, or else this glutton be--\n",
      "to eat the world's due, by the grave and thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(sonnet_list[0])\n",
    "print('Number of words in first sonnet: ', len(tokens))\n",
    "print(tokens[:10]) # 打印前10个token\n",
    "string = detokenize(tokens)\n",
    "print('Reconstructed text:\\n%s' % string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据集中的所有字符串都转化为token列表，并统计所有出现过至少一次的token--这些token将组成模型的词汇表（vocabulary）。\n",
    "\n",
    "然后在词表中添加三个特殊的token：\n",
    "+ `<pad>`: 用于填充序列，使得模型训练时，所有输入序列的长度相同\n",
    "+ `<start>`: 用于标记序列的开始\n",
    "+ `<end>`: 用于标记序列的结束\n",
    "\n",
    "例如，当规定序列的最大长度为8时，对于输入token序列`['hello', 'world']`，我们将预处理为`['<start>', 'hello', 'world', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']`（该预处理操作将会在dataset类中实现）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary: 3115\n",
      "Maximum length of tokens of sonnets in dataset: 169\n"
     ]
    }
   ],
   "source": [
    "sonnets_in_tokens = [] # 每一个元素是一个token列表\n",
    "vocab = set()          # 词表先采用set数据结构来构建，方便去重\n",
    "max_length = 0         # 按token数量来计算，最长的诗的长度\n",
    "for son in sonnet_list:\n",
    "    tokens = tokenize(son)\n",
    "    sonnets_in_tokens.append(tokens)\n",
    "    for tok in tokens:\n",
    "        vocab.add(tok)\n",
    "    length = len(tokens)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "\n",
    "vocab = list(vocab)   # 词表转换为列表，方便索引\n",
    "vocab += ['<PAD>', '<START>', '<END>'] # 添加三个特殊token\n",
    "print('Number of unique words in the vocabulary:', len(vocab))\n",
    "print('Maximum length of tokens of sonnets in dataset:', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token可以转化为自然数形式的索引（id），即其在`vocab`列表中的index，从而形成一一映射。神经网络将以id序列作为输入，并预测下一个token的id，因此为了方便实现，我们将所有的诗歌转化为id序列的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {tok: i for i, tok in enumerate(vocab)} # 用dict结构构造反向索引\n",
    "sonnets_in_ids = []  # 每一个元素是一个id列表\n",
    "for son in sonnets_in_tokens:\n",
    "    ids = []\n",
    "    for tok in son:\n",
    "        # 请完善此处代码，将诗歌数据集中的token转换为id\n",
    "        ids.append(token_to_id[tok])\n",
    "    sonnets_in_ids.append(ids)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当神经网络预测了一个id序列，我们需要一个函数完成从id序列到string的解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but, as the riper should by time decease,\n",
      "his tender heir might bear his memory.\n",
      "but thou, contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thyself thy foe, to thy sweet self too cruel.\n",
      "thou that art now the world's fresh ornament\n",
      "and only herald to the gaudy spring\n",
      "within thine own bud buriest thy content\n",
      "and, tender churl, mak'st waste in niggarding.\n",
      "pity the world, or else this glutton be--\n",
      "to eat the world's due, by the grave and thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode(ids):\n",
    "    tokens = []\n",
    "    for i in ids: \n",
    "        tok = vocab[i]\n",
    "        if tok in ['<START>']:\n",
    "            continue\n",
    "        if tok in ['<END>', '<PAD>']:\n",
    "            break\n",
    "        tokens.append(tok)\n",
    "    string = detokenize(tokens)\n",
    "    return string\n",
    "print(decode(sonnets_in_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据集类定义\n",
    "\n",
    "按照使用PyTorch进行深度学习的惯例，我们会定义一个`Dataset`类，该类继承自`torch.utils.data.Dataset`，并要求实现`__len__`和`__getitem__`方法。然后使用`torch.utils.data.DataLoader`类来提供数据迭代器，该迭代器将自动完成数据的批量化和打乱等操作，并在背后提供了多线程数据加载的功能。建议阅读[资料](https://blog.csdn.net/flyconley/article/details/119119817)。\n",
    "\n",
    "请根据提示完善代码中`__getitem__`方法的定义。注意，当你正确实现了`__getitem__`方法，应当可以通过下面的`assert`检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, sonnets_in_ids, vocab, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.data = sonnets_in_ids\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.pad_id = self.vocab.index('<PAD>')\n",
    "        self.start_id = self.vocab.index('<START>')\n",
    "        self.end_id = self.vocab.index('<END>')\n",
    "        self.max_seq_length = max_seq_length + 2\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"抽取第index个样本，对其进行处理（加上`<START>`，`<END>`和`<PAD>`相应的id），\n",
    "        然后转化为torch.LongTensor（维度为[max_seq_length]），最后返回该tensor\n",
    "        \"\"\"\n",
    "        x = self.data[index]# 取data中的第index个元素\n",
    "        x = [self.start_id] + x + [self.end_id]\n",
    "        x += [self.pad_id] * (self.max_seq_length - len(x))\n",
    "        x = torch.LongTensor(x)# 转化为torch.LongTensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4  # 可以自行调整batch size\n",
    "train_set = dataset(sonnets_in_ids, vocab, max_length)\n",
    "assert train_set[0].shape == torch.Size([max_length + 2])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型定义\n",
    "\n",
    "我们将定义SonNet类，它拟合了条件概率：\n",
    "$$\n",
    "P(y_i|y_{i-1}, y_{i-2}, \\cdots, y_1)\n",
    "$$\n",
    "其中$y_i$表示生成序列中第$i$个token的id，$y_{i-1}$表示第$i-1$个token的id，以此类推。简单来说，输入一个已经生成的id序列，模型将预测下一个序列元素是某id的概率（类似于分类网络中的分类概率）。我们通常使用`softmax`函数将分类线性层的输出（称为logits，值域为$\\mathbb{R}$）转化为概率（值域为$[0, 1]$），但是在实际实现中我们把`softmax`函数操作放在`forward`函数外部，因为：\n",
    "1. 损失函数`nn.CrossEntropyLoss`已经包含了`softmax`函数，因此我们不需要再次调用`softmax`函数\n",
    "2. 方便在采样（文本生成）时对概率的计算进行调整（例如引入`temperature`参数）\n",
    "\n",
    "在训练和生成时，模型具有不同的输入输出形式（但可以共享同一套forward流程）。如下图所示，在训练时（左图），输入为`[0:seq_len-1]`的id序列，输出标签（label）为`[1:seq_len]`的id序列，即相同的序列长度但是向左偏移一个位置。在生成时（右图），一次生成一个id，输出重新作为输入。\n",
    "![lstm](./imgs/lstm.png)\n",
    "阅读[LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)和[Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)的文档，完成`forward`方法的定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonNet(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"完成模型的前向传播过程\n",
    "        输入\n",
    "        inputs: tensor，维度为`[batch_size, max_seq_length]`\n",
    "        hidden: tuple，仅在采样时使用，包含了LSTM的初始隐状态向量和细胞状态向量，见输出`hidden`的说明\n",
    "        输出\n",
    "        outputs: tensor，维度为`[batch_size, max_seq_length, vocab_size]`，值域为$\\mathbb{R}$\n",
    "        hidden: tuple，包含了LSTM的最终隐状态向量和细胞状态向量，仅在采样时使用，一次`forward`的hidden输出作为\n",
    "            下一次forward的`hidden`输入（第一次仍然输入`None`）\n",
    "        \"\"\"\n",
    "        inputs = self.embedding(inputs) # [batch_size, max_seq_length, hidden_size]\n",
    "        if inputs.ndim == 2:\n",
    "            inputs = repeat(inputs, 'a b -> c a b', c=1)\n",
    "        # inputs.to(device_id)\n",
    "        # outputs, self.hidden = nn.LSTM(inputs,self.hidden)\n",
    "        outputs, hidden = self.lstm(inputs, hidden)\n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练\n",
    "\n",
    "实例化模型、优化器和损失函数。可自行调整超参数，例如学习率、隐状态向量维度等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "LR = 0.001\n",
    "EPOCHS = 10  # 可以自行调整训练轮数，建议先设置较少轮数验证实现的准确性，再调整轮数使模型loss收敛\n",
    "model = SonNet(vocab_size=len(vocab), hidden_size=HIDDEN_SIZE, output_size=len(vocab))\n",
    "model.to(device_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_set.pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据最大化似然函数方法，我们要最大化采样（生成）到训练样本的概率，这等价于最小化交叉熵损失函数。如第三节中的图所示，我们可以一次性输入整个序列，并在输出上对每一个位置计算交叉熵损失函数（`nn.CrossEntropyLoss(ignore_index=train_set.pad_id)`会忽略输出label为`<PAD>`的loss）。请完成loss计算和模型权重更新的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "loss_cache = []\n",
    "print(1)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(1)\n",
    "    epoch_loss_sum = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device_id)\n",
    "        inputs = batch[:, :-1].contiguous()\n",
    "        labels = batch[:, 1:].contiguous()\n",
    "        print(inputs,labels)\n",
    "        logits, _ = model(inputs)\n",
    "        # 完成loss的计算，反向传播和权重更新\n",
    "        print(1)\n",
    "        loss = criterion(logits, labels)# .mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(i,loss)\n",
    "        loss_cache.append(loss.item())\n",
    "        epoch_loss_sum += loss.item()\n",
    "    epoch_loss_avg = epoch_loss_sum / len(train_loader)\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, epoch_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\34323\\vs_python\\pytest\\rnn练习\\sonnets_lstm.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/sonnets_lstm.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 画出loss曲线\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/sonnets_lstm.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(loss_cache)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/sonnets_lstm.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mIteration\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/34323/vs_python/pytest/rnn%E7%BB%83%E4%B9%A0/sonnets_lstm.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_cache' is not defined"
     ]
    }
   ],
   "source": [
    "# 画出loss曲线\n",
    "plt.plot(loss_cache)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 序列采样\n",
    "\n",
    "本节没有代码实现任务。请阅读并理解生成代码，实验模型的生成效果并回答相关问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_token='<START>', max_length=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    start_id = token_to_id[start_token]\n",
    "    x = torch.LongTensor([start_id]).to(device_id)\n",
    "    hidden = None\n",
    "    ids = []\n",
    "    logprob = 0.\n",
    "    for i in range(max_length):\n",
    "        logits, hidden = model(x, hidden)\n",
    "        logits = logits.view(-1)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        next_id = next_id.item()\n",
    "        next_prob = probs[next_id]\n",
    "        logprob += torch.log(next_prob)\n",
    "        ids.append(next_id)\n",
    "        x = torch.LongTensor([next_id]).to(device_id)\n",
    "        if next_id == token_to_id['<END>']:\n",
    "            break\n",
    "    string = decode(ids)\n",
    "    return string, logprob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string, logprob = generate(model, temperature=0.3)\n",
    "print('Logprob of generated string:', logprob)\n",
    "print('Generated string:\\n%s' % string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答下列问题：\n",
    "1. 请简要评价`generate`函数的生成效果？有哪些可能的改进（对模型改进、对生成函数的改进均可）？\n",
    "2. 分析并解释参数`temperature`的作用？它是如何影响生成结果的多样性和可靠性的？\n",
    "3. （附加题）`logprob`表示了生成某个字符串的对数概率值，`logprob`越大，则模型认为该字符串有更大的概率生成，字符串也就越可靠（语句更通顺，更接近训练样本的语言风格）。给定一个已训练好的模型，怎样改进`generate`函数可以使得生成字符串的`logprob`更大？实现你所描述的改进可直接获得本次作业的满分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "944c5db5ed3f6a139be1a1ca00c912d16f8d9b9fe488d18ee7534a98d95d0674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
