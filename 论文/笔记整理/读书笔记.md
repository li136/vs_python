# AlexNet
## 特点
- 使用多层带maxpooling的卷积层
- 使用relu激活函数，训练更快
- 使用dropout减少过拟合

## 知识点
- 使用ReLUs的深度卷积神经网络的训练速度比使用tanh的深度卷积神经网络快数倍。
- GPU并行运算：将一半的内核（或神经元）放在每个GPU上，GPU只在某些层进行通信
- label-preserving transformations

## 疑问
prior knowledge    
# VGGNet
## 特点
使用多个小卷积层比如3*3卷积，堆深度

## 知识点
- 使用三个3×3层具有7×7的有效感受野（Receptive Field），相比于单个7×7层：
加入了三个非线性校正层，而不是一个非线性校正层，这使得决策函数更具区分性。
减少了参数的数量
- 1×1卷积层是一种在不影响感受野的情况下增加网络非线性的方法。
- 用浅的网络随机初始化，进行训练，再将训练后的参数作为深的网络的部分初始化值
- 多GPU训练利用数据并行性，通过将每一批训练图像分割成几个GPU批，在每个GPU上并行处理来执行。计算GPU批次梯度后，对其进行平均，以获得整个批次的梯度。梯度计算在GPU之间是同步的，因此结果与在单个GPU上训练时完全相同。
- 对几个模型的软最大类后验概率进行平均，来组合这些模型的输出。由于模型的互补性，这提高了性能

## 疑问
- scale jittering：
crop size是固定的，而image size是随机可变的。举例来说，比如把crop size固定在224×224，而image的短边可以按比例缩放到[256, 480]区间的某个随机数值，然后随机偏移裁剪个224×224的图像区域。
# ResNet
## 特点
- 对每层的输入做一个reference, 学习形成残差函数， 而不是学习一些没有reference的函数。这种残差函数更容易优化，能使网络层数大大加深。  
残差函数理解为残差网络更贴切一点
- 残差网络（Residual Network）是一种非常有效的缓解梯度消失问题网络，极大的提高了可以有效训练的网络的深度。

## 知识点
- 随着网络深度的增加，精度会饱和，然后迅速退化。
-  对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function），新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。  
后半句不能理解
- 使用1*1卷积减少通道数，再卷积，再用1*1卷积增加通道数
- shortcut connections
-     if F has only a single layer,is similar to a linear layer: y = W1x + x   
-     Let us consider H(x) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net),with x denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions^2, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., H(x) − x (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) − x. The original function thus becomes F(x)+x. Although both forms should be able to asymptotically approximate the desired functions (as hypothesized),the ease of learning might be different.     残差原理，不翻译直接看原文
-     We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3. The reason for such optimization difficulties will be studied in the future. 作者认为网络退化不是梯度消失的问题，但现在还是会认为是resnet梯度保持的比较好

## 疑问
- 为什么深的网络会退化？？？
因为梯度消失梯度爆炸不能被完美解决？
-     If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.
- resnet网络模型和我先训练一个浅层模型，多次加新的层（不改变已训练好的浅层参数）重新训练，以及加新层（旧参数仅作为初始参数）全部重新训练。在结果上有什么差别
- 感觉残差不贴切模型原意
- identity mappings，恒等映射，在一个浅层网络基础上叠加y=x的层
- F(x) = H(x) - x  
F(x)、H(x)、F(x)是啥？  
H(x)是原始网络、F(x)是残差网络、x是上一层的输入
- 深度增加，训练速度为什么会比VGG快。因为全是卷积吗？
- auxiliary classifiers
- “inception” layer
# DenseNet
## 特点
- 基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection）。
- 通过特征在channel上的连接来实现特征重用（feature reuse）。
## 知识点
- resnet函数之间有relu函数，不是完全短路连接
# Adam
## 特点
- RMSProp、AdaGrad、Momentum
- ![公式](https://upload-images.jianshu.io/upload_images/10046814-59e992b67938aec9.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)
- RMSProp算法，加权均值，调整v和m
## 知识点
- Momentum将梯度作为加速度，使过去梯度信息也会影响现在的学习率，详见mt
- AdaGrad使用梯度平方累加的方法，计算每个参数过去调整的值的平方的总和。总和越大的参数就少动点。见公式中的vt
- RMSporp使用加权均值，调整vt和mt的值，慢慢遗忘过去的梯度信息

<img src="img/IMG_20220622_155045.jpg" width="50%"><img src="img/IMG_20220622_155052.jpg" width="50%">
# BatchNorm和LayerNorm
## 特点
- 在卷积层，作用在通道层。  
在全连接层，作用在特征层。
- 对x1到xm，先求xi的均值和方差，再对x归一化，再加入可学习参数a、b，  yi=a*xi + b
- 现在主流认为它就是通过在每个小批量里加入噪音来控制模型复杂度，和dropout效果有点重
- 可以用来加速收敛，一定程度缓解梯度爆炸、消失。不改变最终模型精度
## 知识点
<div align=center><img src="img\1655947104389.jpg" width="50%" >  </div>

- BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的所有特征做归一化.  
如果具体任务依赖于不同样本之间的关系，BN更有效，尤其是在CV领域，例如不同图片样本进行分类，不同样本之间的大小关系得以保留。  
LN更适合NLP领域的任务，其中，一个样本的特征实际上就是不同word embedding，通过LN可以保留特征之间的这种时序关系。
# GAN
## 特点
- 两个网络对抗，进步
## 知识点
- 生成器用随机噪音生成虚假的结果。  
判别器对虚假结果和真实结果做预测  
- 使用adam做优化
- 生成器loss：log(1-pro_atrist1)  
判别器loss： - (log(pro_atrist0)+log(1-pro_atrist1))  
pro_atrist0为判别器对真实结果的打分结果，pro_atrist1为判别器对虚假结果的打分结果  
# RCNN和Faster RCNN和Mask RCNN
## RCNN
- 先用CNN对整个原始图片抽取特征，再使用Rol池化层对每个锚框产生固定长度的特征。再加全连接，做预测。

## Faster RCNN
- 用启发式搜索获得更好的锚框
## Mask RCNN
- 在faster rcnn做完Rol后，如果数据集有像素级别的标号，用fcn利用这些信息
# seq2seq
## 特点
- 用rnn作为编码器读取输入句子（可以是双向）。再用另一个rnn输出。编码器最后输出的隐状态作为编码器的初始隐状态
- 
## 知识点
- decoder训练时，上一时刻单词的输入用正确的输入（而不是之前推理出的结果）和state推出下一结果。
- 用bleu衡量生成序列的好坏。  
对标签序列abcdef和预测序列abbcd  
p1=4/5 p2=3/4 p3=1/3 p4=0  
bleu计算公式为exp(min(0, 1-len(label)/len(pred))) * 累乘(pn^(1/**2^n**))  
1-len(label)/len(pred)惩罚过短的预测，pn^(1/**2^n**)使长匹配有更高权重
- 通过零值化屏蔽不相关的项，获得正确的loss
# Transformer
## 特点
- Self-Attention是 Transformer 的重点
- 纯基于注意力
- 使用编码器-解码器架构处理序列对
## 知识点
- 多头注意力：对同样的key，value，query，抽取不同的信息。  
例如短距离关系和长距离关系  
使用h个独立的注意力池化，然后合并输出得到最终输出  
- 位置编码Positional Encoding，添加位置信息，因为attention会破坏忽略原来的位置信息
- encoding的input embedding后要除num_hiddens根号，再加上位置编码Positional Encoding
- 层归一化
- 基于位置的前馈网络
- 编码器的输出y1-yn作为解码器中第i个tramsformer块中的key和value。query来自目标序列
## 代码
EncoderBlock

    class EncoderBlock(nn.Module):
        forward:
            input   ->  MultiHeadAttention
                    ->  AddNorm(残差)
                    ->  FFN
                    ->  AddNorm(残差)
TransformerEncoder

    class TransformerEncoder(Encoder):
        forward:
            input   ->  embedding
                    ->  pos_encoding(位置编码)
                    嵌入值乘以嵌入维度的平方根进行缩放，再与位置编码相加。
                    ->  EncoderBlock，同时更新attention_weights
DecoderBlock

    <!-- 训练阶段，输出序列的所有词元都在同一时间处理， -->
    class EncoderBlock(nn.Module):
        forward:
            input   ->  自注意力attention
                    ->  AddNorm(残差)
                    ->  编码器－解码器注意力（kv来自Encoder）
                    ->  addnorm2
                    ->  FFN
                    ->  AddNorm(残差)
TransformerDecoder

    class TransformerEncoder(AttentionDecoder):
        forward:
            input   ->  embedding
                    ->  pos_encoding(位置编码)
                    ->  DecoderBlock，同时更新attention_weights
## 疑问
<div align=center><img src="https://pic4.zhimg.com/v2-f6380627207ff4d1e72addfafeaff0bb_r.jpg" width="50%"> </div> 

# BERT
## 特点
- 基于微调的nlp模型，新的任务只需要增加一个简单的输出层         
- 只有编码器的transformer
- 预训练任务：1、下一句子预测是否相邻  2、带掩码的语言模型
## 知识点
- 10%的词会被替换成随机次元的原因：
作者在论文中谈到了采取上面的mask策略的好处。大致是说采用上面的策略后，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。
- 迁移学习
# Show, attend and tell
## 特点
- 用cnn作为encoder，用attention和LSTM作为decoder
- 图像转文字
## 知识点
- 输入图像归一化到224 × 224   
使用现成的VGG网络部分架构，输出为conv5_3层的14×14×512维特征。  
用attention
再通过一层全连接层作为transformer作为LSTM的第一个hidden state，再用LSTM得到输出句子y
# 代码
两个三维向量，但是第二维不一样，用广播机制变成两个四维向量相加  

    features = queries.unsqueeze(2) + keys.unsqueeze(1)
    # queries的形状：(batch_size，num of queries，1，num_hidden)
    # key的形状：(batch_size，1，num of keys，num_hiddens)

    #不同维度tensor相加
    a=torch.ones([2,1,8])
    b=torch.ones([2,10,8])
    c=a+b
    print(c.shape)
---
    assert len(X.shape) in (2, 4)
    #如果X.shape是2或4的话，继续执行，否则报错
---
    modules = list(resnet.children())[:-2]
    #移除linear and pool layers
---
    #微调
    def fine_tune(self, fine_tune=True):
        """
        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.

        :param fine_tune: Allow?
        """
        for p in self.resnet.parameters():
            p.requires_grad = False
        # If fine-tuning, only fine-tune convolutional blocks 2 through 4
        for c in list(self.resnet.children())[5:]:
            for p in c.parameters():
                p.requires_grad = fine_tune
---
    X = torch.ones((2, 1, 4))
    Y = torch.ones((2, 4, 6))
    # bmm也是矩阵乘法，2是批量
    print(torch.bmm(X, Y).shape)
---
    #测试repeat_interleave和matmul
    a=torch.tensor([1.0,2.0,3.0])
    b=a.repeat_interleave(3).reshape((-1, 3))
    print(b)
    c=b - torch.tensor([1.0,2.0,3.0])
    print(c)
    print(nn.functional.softmax(c,dim=1))
    print(torch.matmul(b - torch.tensor([1.0,2.0,3.0]), a))
---
    #unsqueeze()拓展维度，squeeze()对数据的维度进行压缩
    print(weights.unsqueeze(1).shape)