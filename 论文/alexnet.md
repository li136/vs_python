## 特点
- 使用多层带maxpooling的卷积层
- 使用relu激活函数，训练更快
- 使用dropout减少过拟合

## 知识点
- 使用ReLUs的深度卷积神经网络的训练速度比使用tanh的深度卷积神经网络快数倍。
- GPU并行运算：将一半的内核（或神经元）放在每个GPU上，GPU只在某些层进行通信
- label-preserving transformations

## 疑问
specified

prior knowledge是啥

feedforward neural network前馈神经网络

第二种形式的数据增强包括改变训练图像中RGB通道的强度

