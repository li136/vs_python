## 独热向量是一个糟糕的选择
- 虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度
- 由于每一个单词的词向量的维度都等于词汇表的长度，对于大规模语料训练的情况，词汇表将异常庞大，使模型的计算量剧增造成维数灾难。
- 有用的信息零散地分布在大量数据中。这会导致结果异常稀疏，使其难以进行优化，对于神经网络来说尤其如此。
## 自监督的word2vec
- word2vec工具是为了解决上述问题而提出的。它将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系。
## CBOW（连续词袋）模型
- CBOW(continuous bag of words)模型的核心思想是：在一个句子中遮住目标单词，通过其前面以及后面的单词来推测出这个单词w。首先规定词向量的维度V，对数据中所有的词随机赋值为一个V维的向量，每个词向量乘以参数矩阵W(VN维矩阵)，转换成N维数据，然后要对窗口范围内上下文的词向量相加取均值作为输入层输入到隐藏层，隐藏层将维度拉伸后全连接至输出层然后做 一个softmax的分类从而预测目标词。最终用预测出的w与真实的w作比较计算误差函数，然后用梯度下降调整参数矩阵。
## skip-gram（跳字）模型
- skip-gram模型的核心思想是：模型根据目标单词来推测出其前面以及后面的单词。它的模型结构与CBOW正好相反，只不过它的输入是目标词，输出是目标词的邻接词，从模型结构示意图上看相当于输入层与输出层交换位置，先将目标词词向量映射到投影层，再将投影层的输出作为输出层的输入，最后预测目标词窗口范围内的邻接词。
## 下采样
- 文本数据通常有“the”、“a”和“in”等高频词：它们在非常大的语料库中甚至可能出现数十亿次。然而，这些词经常在上下文窗口中与许多不同的词共同出现，提供的有用信息很少。例如，考虑上下文窗口中的词“chip”：直观地说，它与低频单词“intel”的共现比与高频单词“a”的共现在训练中更有用。此外，大量（高频）单词的训练速度很慢。因此，当训练词嵌入模型时，可以对高频单词进行下采样 [Mikolov et al., 2013b]。具体地说，数据集中的每个词将有概率地被丢弃
- 我们可以看到，只有当单词相对总词数比率大于超参数t时，（高频）词才能被丢弃，且该词的相对比率越高，被丢弃的概率就越大。
- 下采样通过删除高频词来显著缩短句子，这将使训练加速。