## 特点
- 在卷积层，作用在通道层。  
在全连接层，作用在特征层。
- 对x1到xm，先求xi的均值和方差，再对x归一化，再加入可学习参数a、b，  yi=a*xi + b
- 现在主流认为它就是通过在每个小批量里加入噪音来控制模型复杂度，和dropout效果有点重
- 可以用来加速收敛，一定程度缓解梯度爆炸、消失。不改变最终模型精度
## 知识点
![avatar](笔记整理\img\1655947104389.jpg)
- BN是对一个batch-size样本内的每个特征做归一化，LN是对每个样本的所有特征做归一化.  
如果具体任务依赖于不同样本之间的关系，BN更有效，尤其是在CV领域，例如不同图片样本进行分类，不同样本之间的大小关系得以保留。  
LN更适合NLP领域的任务，其中，一个样本的特征实际上就是不同word embedding，通过LN可以保留特征之间的这种时序关系。
## 疑问
- 
- 