## 特点
使用多个小卷积层比如3*3卷积，堆深度

## 知识点
- 使用三个3×3层具有7×7的有效感受野（Receptive Field），相比于单个7×7层：
加入了三个非线性校正层，而不是一个非线性校正层，这使得决策函数更具区分性。
减少了参数的数量
- 1×1卷积层是一种在不影响感受野的情况下增加网络非线性的方法。
- 用浅的网络随机初始化，进行训练，再将训练后的参数作为深的网络的部分初始化值
- 多GPU训练利用数据并行性，通过将每一批训练图像分割成几个GPU批，在每个GPU上并行处理来执行。计算GPU批次梯度后，对其进行平均，以获得整个批次的梯度。梯度计算在GPU之间是同步的，因此结果与在单个GPU上训练时完全相同。
- 对几个模型的软最大类后验概率进行平均，来组合这些模型的输出。由于模型的互补性，这提高了性能

## 疑问
- scale jittering：
crop size是固定的，而image size是随机可变的。举例来说，比如把crop size固定在224×224，而image的短边可以按比例缩放到[256, 480]区间的某个随机数值，然后随机偏移裁剪个224×224的图像区域。
- dense ConvNet是啥