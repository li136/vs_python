## 特点
- 只有编码器的transformer

## 知识点
- 10%的词会被替换成随机次元的原因：
作者在论文中谈到了采取上面的mask策略的好处。大致是说采用上面的策略后，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。
- 

## 疑问
- 