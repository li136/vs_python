## 特点
- 基于微调的nlp模型，新的任务只需要增加一个简单的输出层         
- 只有编码器的transformer
- 预训练任务：1、下一句子预测是否相邻  2、带掩码的语言模型
## 知识点
- 10%的词会被替换成随机次元的原因：
作者在论文中谈到了采取上面的mask策略的好处。大致是说采用上面的策略后，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。
- 迁移学习
- segment embeddings区分一个句子对中的两个句子。token embedding 层是要将各个词转换成固定维度的向量。加入position embeddings会让BERT理解下面下面这种情况, I think, therefore I am,第一个 “I” 和第二个 “I”应该有着不同的向量表示
- 双向
## 疑问
- 