- CLIPBERT，一个通用而有效的端到端视频和语言学习框架
```bash
处理这些跨模式任务的实际范例是，首先从预训练的视觉模型中提取密集的视频特征，并从预训练的语言模型中提取文本特征，然后应用多模式融合在共享的嵌入空间中将这些固定表示形式结合在一起
遵循这一范式的现有方法取得了巨大成功，但存在两个主要缺点
# tasks/domains中的断开连接：offline feature extractors通常针对不同于目标任务的tasks/domains进行训练。例如，从人类活动视频中学习的动作识别特征不一致地应用于generic-domain GIF视频上的下游视频问答。
# 多模式特征中的断开：从不同模式中学习的特征相互独立。例如，动作识别模型通常是从纯视频数据中训练出来的，没有文本输入，但应用于视频和语言任务。

端到端特定于任务的微调提供了一种缓解这些固有断开的方法。

然而，与大多数现有工作一样，从完整的视频帧序列中提取特征会对内存和计算造成过度需求，因此很难甚至不可能直接将特征提取器插入到视频+语言学习框架中以实现有效的端到端微调。
```
```bash
仅从视频中得到一个或者极少的clip用于训练（之前基本上是densely sampling）
```
![avatar](笔记整理\img\clipbert1.jpg)